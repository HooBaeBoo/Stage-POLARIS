# -*- mode: org -*-
# -*- coding: utf-8 -*-
#+STARTUP: overview indent inlineimages logdrawer
#+TITLE:       Journal
#+AUTHOR:      EZ-ZINE Najwa
#+LANGUAGE:    en, fr
#+TAGS: LIG(L)
#+TAGS:  OrgMode(O)
#+EXPORT_SELECT_TAGS: Blog
#+OPTIONS:   H:3 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:nil skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+COLUMNS: %25ITEM %TODO %3PRIORITY %TAGS
#+SEQ_TODO: TODO(t!) STARTED(s!) | DONE(d!) CANCELLED(c!)

* 2018
TIP : Git add commit and push (src:StackOverflow)
#+BEGIN_SRC sh
  alias gacp='echo "Enter commit message -> " && read MSG && git add . && git commit -m "$MSG" && git push'
#+END_SRC
** 2018-02 March
*** 2018-02-12 Monday

**** [[https://github.com/alegrand/RR_webinars/blob/master/1_replicable_article_laboratory_notebook/index.org][First webinar on reproducible research: litterate programming]]
***** Emacs shortcuts
Here are a few convenient emacs shortcuts for those that have never
used emacs. In all of the emacs shortcuts, =C=Ctrl=, =M=Alt/Esc= and
=S=Shift=.  Note that you may want to use two hours to follow the emacs
tutorial (=C-h t=). In the configuration file CUA keys have been
activated and allow you to use classical copy/paste (=C-c/C-v=)
shortcuts. This can be changed from the Options menu.
  - =C-x C-c= exit
  - =C-x C-s= save buffer
  - =C-g= panic mode ;) type this whenever you want to exit an awful
    series of shortcuts
  - =C-Space= start selection marker although selection with shift and
    arrows should work as well
  - =C-l= reposition the screen
  - =C-_= (or =C-z= if CUA keys have been activated)
  - =C-s= search
  - =M-%= replace
  - =C-x C-h= get the list of emacs shortcuts
  - =C-c C-h= get the list of emacs shortcuts considering the mode you are
    currently using (e.g., C, Lisp, org, ...)
  There are a bunch of cheatsheets also available out there (e.g.,
  [[http://www.shortcutworld.com/en/linux/Emacs_23.2.1.html][this one for emacs]] and [[http://orgmode.org/orgcard.txt][this one for org-mode]] or this [[http://sachachua.com/blog/wp-content/uploads/2013/05/How-to-Learn-Emacs-v2-Large.png][graphical one]]).
***** Org-mode                                                  :OrgMode:
  Many emacs shortcuts start by =C-x=. Org-mode's shortcuts generaly
  start with =C-c=.
  - =Tab= fold/unfold
  - =C-c c= capture (finish capturing with =C-c C-c=, this is explained on
    the top of the buffer that just opened)
  - =C-c C-c= do something useful here (tag, execute, ...)
  - =C-c C-o= open link
  - =C-c C-t= switch todo
  - =C-c C-e= export
  - =M-Enter= new item/section
  - =C-c a= agenda (try the =L= option)
  - =C-c C-a= attach files
  - =C-c C-d= set a deadl1ine (use =S-arrows= to navigate in the dates)
  - =A-arrows= move subtree (add shift for the whole subtree)
***** Org-mode Babel (for literate programming)                 :OrgMode:
  - =<s + tab= template for source bloc. You can easily adapt it to get
    this:
    #+BEGIN_EXAMPLE
      #+begin_src shell
      ls
      #+end_src
    #+END_EXAMPLE
    Now if you =C-c C-c=, it will execute the block.
    #+BEGIN_EXAMPLE
  #+RESULTS:
  | #journal.org# |
  | journal.html  |
  | journal.org   |
  | journal.org~  |
    #+END_EXAMPLE

  - Source blocks have many options (formatting, arguments, names,
    sessions,...), which is why I have my own shortcuts =<b + tab= bash
    block (or =B= for sessions).
    #+BEGIN_EXAMPLE
  #+begin_src shell :results output :exports both
  ls /tmp/*201*.pdf
  #+end_src

  #+RESULTS:
  : /tmp/2015_02_bordeaux_otl_tutorial.pdf
  : /tmp/2015-ASPLOS.pdf
  : /tmp/2015-Europar-Threadmap.pdf
  : /tmp/europar2016-1.pdf
  : /tmp/europar2016.pdf
  : /tmp/M2-PDES-planning-examens-janvier2016.pdf
    #+END_EXAMPLE
  - I have defined many such templates in my configuration. You can
    give a try to =<r=, =<R=, =<RR=, =<g=, =<p=, =<P=, =<m= ...
  - Some of these templates are not specific to babel: e.g., =<h=, =<l=,
    =<L=, =<c=, =<e=, ...
***** In case you want to play with ipython/jupyter on a recent debian :Python:
Here is what you should install:
#+begin_src shell :results output :exports both
sudo apt-get install jupyter-notebook python3-matplotlib python3-numpy
#+end_src

On my machine, I got the version 5.2.2:
#+begin_src shell :results output :exports both
jupyter-notebook --version
#+end_src

#+RESULTS:
: 5.2.2

The ipython notebook can then be run with the following command:
#+begin_src shell :results output :exports both
jupyter-notebook
#+end_src

If you also want to use R in jupyter, follow
[[https://github.com/IRkernel/IRkernel#installation][these instructio
*****
***** In case you want to play with R/knitR/rstudio:                  :R:
Here is what you should install on debian:
#+BEGIN_SRC shell
sudo apt-get install r-base r-cran-ggplot2 r-cran-knitr
#+END_SRC
Alternatively, if the installation of =r-cran-gplot2= or =r-cran-knitr=
fails, you may want to install them locally and manually by running
the following commands in R (or Rstudio):
#+BEGIN_SRC R
install.packages("knitr")
install.packages("ggplot2")
#+END_SRC

Rstudio is unfortunately not packaged within debian so the easiest is
to download the corresponding debian package on the [[http://www.rstudio.com/ide/download/desktop][Rstudio webpage]]
and then to install it manually (depending on when you do this, you
can obviously change the version number). Here is how to install it on
an old (stable) debian or ubuntu:
#+BEGIN_SRC shell
cd /tmp/
wget https://download1.rstudio.org/rstudio-1.1.423-amd64.deb
sudo dpkg -i rstudio-1.1.423-amd64.deb
sudo apt-get -f install # to fix possibly missing dependencies
#+END_SRC
If you're running a more recent (testing or unstable) debian, you'll
probably want to use another debian package:
#+BEGIN_SRC shell
cd /tmp/
wget https://download1.rstudio.org/rstudio-xenial-1.1.423-amd64.deb
sudo dpkg -i rstudio-xenial-1.1.423-amd64.deb
#+END_SRC

** 2018-04 April
*** 2018-04-30 Monday
**** OBJECTIVES :
***** TODO Intro : [[https://www.grid5000.fr/mediawiki/index.php/Grid5000:Home][Grid5000]] [33%]
****** DONE Request Account
****** STARTED First grasp
:LOGBOOK:
- State "STARTED"    from "TODO"       [2018-04-30 lun. 17:57]
:END:
******* When using Grid'5000, you will typically:
- connect, using SSH, to an access machine
- connect from this access machine to a site frontend
- on this site frontend, reserve resources (nodes), and connect to those nodes
****** STARTED [[http://execo.gforge.inria.fr/doc/latest-stable/][Execo]]
:LOGBOOK:
- State "STARTED"    from "TODO"       [2018-05-02 mer. 15:42]
:END:
***** DONE Intro : [[https://github.com/alegrand/RR_webinars/blob/master/1_replicable_article_laboratory_notebook/index.org][Org-mode & Emacs]] [100%]
****** DONE install Emacs
*REMINDER* To launch using a specific configuration [[https://raw.githubusercontent.com/alegrand/RR_webinars/master/1_replicable_article_laboratory_notebook/init.el][init.el]]
#+BEGIN_SRC sh
emacs -q -l init.el
#+END_SRC
****** DONE install Org-mode + necessary tools
****** DONE identify key Shortcuts
****** DONE Upload Journal on [[https://github.com/HooBaeBoo/Stage-POLARIS][GitHub]]
***** TODO Discovering the topic[0%] :
****** STARTED Readings [1/2]
:LOGBOOK:
- State "STARTED"    from "TODO"       [2018-04-30 lun. 17:56]
:END:
- [X] [[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.471.6593&rep=rep1&type=pdf][Parallel and distributed simulation systems]] Chapter 6 : Time //
  Simulation [[https://www.acm-sigsim-mskr.org/Courseware/Fujimoto/Slides/FujimotoSlides-16-TimeParallelSimulations1.pdf][Slides]]1 [[https://www.acm-sigsim-mskr.org/Courseware/Fujimoto/Slides/FujimotoSlides-17-TimeParallelSimulations2.pdf][Slides2]]
- [ ] Parallel algorithms A. Legrand, H. Casanova & Y. Robert
***** TODO Intro : SimGrid[66%]
****** DONE [[http://simgrid.gforge.inria.fr/simgrid/latest/doc/install.html][Install]]
****** DONE [[http://simgrid.gforge.inria.fr/tutorials.php][First grasp]]
:LOGBOOK:
- State "DONE"       from "STARTED"  [2018-05-03 jeu. 10:13]
- State "STARTED"    from "TODO"       [2018-05-02 mer. 9:27]
:END:
****** STARTED Basics [[https://www.edx.org/course/introduction-to-python-absolute-beginner][Python & Jupyter]]
:LOGBOOK:
- State "STARTED"    from "TODO"       [2018-04-30 lun. 17:56]
:END:
**** Readings Synthesis :
#+BEGIN_VERSE
In order to simulate a system one could think of several solutions. In
fact, there are 2 in particular we could describe : \\
*SPATIAL DECOMPOSTITION* : Each logical process is to compute all
variables' values within the simulation time. This type of
decomposition will not be treated, we will focus on the following 2nd type.\\
*TEMPORAL DECOMPOSITION* : In this solution, time is first divided into
smaller chunks and each logical process is assigned a time chunk to
work on. Here, processes are expected to perform a simulation of the
entire system. This method however creates a problem. It wouldn't make
sense to jump from one state to another with no continuity. So we
expect those logical processes to come up with matching boundary
values. When simulating to know how a process is going to end I need
to simulate it. But that would destroy everything. We call this the
state-matching problem and scientists came up with 3 differents
solving approaches. First, the /fix-up computation/ : logical processes
guess randomly the final state of the previous process. If it was
wrong, it will just start over again with the right initial
state. Second and third, the /precomputation of state at specific time division
points/ and /parallel prefix computation/: If we know our system very
well or if I have an equation, we could use this data to
predict their behavior.
#+END_VERSE
** 2018-05 May
*** 2018-05-02 Wednesday
**** SimGrid :
- MSG : Simple application-level simulator
- SimDag : Directed Acyclic Graph (DAG), we assume vertices represent tasks and edges represents constraints and/or data movements between tasks.
We then use it to know what happened with the tasks. Who did what. It's very useful when simulation parallel application. See example ex1-2.c.
To describe the graph we can either directly detail them in the c file or use the 2 loaders SimDag comes with :
DAX or DOT. While DAX is a xml file to fill with general information such as tasks' names, I/O files, DOT format is a bit more human-friendly. DOT format will have you describe the graph with arrows, brackets. A more intuitive approach.


#+CAPTION: example of DAX file
file:Ressources/img/DAX.png

#+CAPTION: example of DOT file
[[file:Ressources/img/DOT.png]]

*REMINDER* How to call the loader : ~SD_daxload(filename) / SD_dotload(filename)~
- Platform : /Understood how it works and what it represents but have no
  clue what it's for. Feel like I'm missing the point./
  - XML based description
  - Lua based description
- SMPI : Reimplementation of MPI on top of SimGrid. The laptop perform all computations with faked communications.
[[https://www.ibm.com/support/knowledgecenter/en/SSZTET_10.1.0/smpi02/smpi02_host_list.html][Hostfile explanation]]
- MPI : Message Passing Interface (MPI)
#+BEGIN_QUOTE
A communication protocol for programming parallel computers.
#+END_QUOTE
(src : Wikipedia)
*** 2018-05-03 Thursday
**** [[https://www.grid5000.fr/mediawiki/index.php/Execo_Practical_Session#Overview][Execo]] & [[https://www.grid5000.fr/mediawiki/index.php/Getting_Started][Grid5000]]:
/Note/ : Sample configuration file not found but there are config.py files.
- Execo : Python API for controling unix process (local/remote, standalone/parallel).
- Execo_ g5k : Set of tools and extensions for Grid5000 testbed.
- Execo engine : tools to ease development of computer sciences experiments
***** Conduct :
We start by checking the installation using iPython tunning a simple 'Hello world test'
#+BEGIN_SRC sh
ipython
import execo
execo.Process("echo 'hello world').run().stdout
out : hello world
#+END_SRC
- We're about to need Grid'5000. We launch it.
#+BEGIN_SRC sh
 ssh nezzine@access.grid5000.fr
#+END_SRC
*Problem encountered* : Permission denied (publickey).\\
*Solution* : Create a key and add it to Grid'5000 Account

*** MEETING NOTES FROM F.PERRONNIN & A.LEGRAND :
**** Discussions avec Florence et Arnaud
***** Notebook jupyter:
****** Install
Surtout ce premier bout de code
#+begin_src sh :results output :exports both
sudo apt-get install jupyter-notebook
sudo apt-get install python3-pip
sudo apt-get install python3-matplotlib python3-numpy
#+end_src

Then following https://github.com/kirbs-/hide_code (note sure this is
as useful as I thought though :()
#+begin_src sh :results output :exports both
sudo pip3 install hide_code
sudo jupyter-nbextension  install --py hide_code
jupyter-nbextension  enable --py hide_code
jupyter-serverextension enable --py hide_code
#+end_src

Pour que l'export via latex fonctionne:
#+begin_src shell :results output :exports both
sudo apt-get install wkhtmltopdf
sudo apt-get install texlive-xetex
#+end_src

Pour avoir R:
#+begin_src shell :results output :exports both
sudo apt-get python3-rpy2
#+end_src

Pour avoir le [[https://github.com/brospars/nb-git][git push/pull dans
les notebooks]]:
#+begin_src shell :results output :exports both
jupyter nbextension install
https://raw.githubusercontent.com/brospars/nb-git/master/nb-git.js
jupyter nbextension enable nb-git
#+end_src

Autres extensions (code-folding):
https://stackoverflow.com/questions/33159518/collapse-cell-in-jupyter-notebook
#+begin_src shell :results output :exports both
pip3 install jupyter_contrib_nbextensions
# jupyter contrib nbextension install --user # not done yet
#+end_src

https://stackoverflow.com/questions/33159518/collapse-cell-in-jupyter-notebook
(collapsible headings)

Pour avoir le kernel R (from https://irkernel.github.io/installation/):
#+begin_src R :results output graphics :file (org-babel-temp-file
"figure" ".png") :exports both :width 600 :height 400 :session *R*
install.packages(c('repr', 'IRdisplay', 'evaluate', 'crayon', 'pbdZMQ',
'devtools', 'uuid', 'digest'))
devtools::install_github('IRkernel/IRkernel')
# Don’t forget step 2/2!
IRkernel::installspec()
#+end_src

****** Export
http://markus-beuckelmann.de/blog/customizing-nbconvert-pdf.html
https://nbconvert.readthedocs.io/en/latest/

#+begin_src sh :results output :exports both
ipython3 nbconvert --to pdf Untitled.ipynb
#+end_src
****** Pour aller plus loin
- https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/
***** Expériences EXECO:
  - https://www.grid5000.fr/mediawiki/index.php/BigData_hands-on_tutorial
  -
https://gitlab.inria.fr/grid5000/bigdata-tutorial/blob/master/Experiment.ipynb
**** ssh et screen
Dans ton ~.ssh/config~:
#+BEGIN_EXAMPLE
#######################################
# Config globale

#Host *
ForwardAgent yes
ForwardX11 yes

Host *.g5k
   User alegrand
   ProxyCommand ssh alegrand@access.grid5000.fr "nc  -w 60 `basename %h
.g5k` %p"
   RemoteForward 1947 scm.gforge.inria.fr:22
#+END_EXAMPLE

Bien utiliser =screen= pour avoir des codes distants persistants:
- =C-a C-d= pour détacher le screen
- =screen -a -r= pour le rattacher quand on revient
**** SimGrid
- examples/smpi/NAS/ep.c
- examples/platforms/*.xml

*** MEETING SYNTHESIS:
- Jupyter allows taking notes similarly to RStudio. We can dedicate cells to a specific language.
*CAREFUL* : Avoid falling into easy traps like x = x + 10.
- Python : it allows drawing graphs again similarly to r with its plot function.
- Grid5000 : RECOMMENDED CONFIGURATION :
  - **** ssh et screen
Dans ton ~.ssh/config~:
#+BEGIN_EXAMPLE
#######################################
# Config globale

#Host *
ForwardAgent yes
ForwardX11 yes

Host *.g5k
   User alegrand
   ProxyCommand ssh alegrand@access.grid5000.fr "nc  -w 60 `basename %h
.g5k` %p"
   RemoteForward 1947 scm.gforge.inria.fr:22
#+END_EXAMPLE

- SimGrid :In the future, we will focus on SMPI/MPI.
  - SIMIX : creating processes
  - MPI : Communication protocol send/receive, include smart types (example : if I want a column from a matrix, considering the matrix linear organization, I need to send regularly-spaced values), there's also broadcast
  - SMPI : is just a simulation. When I do Smpicc followed by smpirun, things will be excecuted locally. How?
    - I can describe the plateform using a xml file.
    - *Advise* : Take a look at NAS.
- *Clue* : to put in place the time parallelization will consist in running with smpirun and all settings needed and I will need to state for each process : when to start. [Nicol's algorithm]
- Tools that could be needed in the future : /screen/ (for persitance).
*** 2018-05-04 Friday :
**** Installation Python & Jupyter :
#+BEGIN_SRC sh
sudo apt-get install python3-pip
sudo apt-get install python3-matplotlib python3-numpy
sudo apt-get install jupyter-notebook
#+END_SRC
*Problem encountered* : package jupyter-notebook doesn't exists
*Solution* :
#+BEGIN_SRC sh
pip install --user jupyter
#+END_SRC
/Note/ : --user is used to install in a directory that doesn't requires root permissions.
*CAREFUL* Avoid doing :
#+BEGIN_SRC sh
pip install --upgrade pip
#+END_SRC
as it creates [[https://github.com/pypa/pip/issues/5221][damage]].
**** Configuration .ssh/config
/Cf Meeting with Florence & Arnaud/
*Problem encountered* : X11 forwarding request failed on channel 0
*Solution* : PENDING, tried : [[https://www.cyberciti.biz/faq/how-to-fix-x11-forwarding-request-failed-on-channel-0/][this]] : Unauthorized actions, will be reported.
Retried : Worked! You'll find a template in ~Ressources/templates/ssh_config~
**** [[https://www.grid5000.fr/mediawiki/images/G5k_cheat_sheet.pdf][Grid5000 Key commands]] :
***** Basics :
- To connect :
#+BEGIN_SRC sh
ssh login@access.grid5000.fr
OR (if ssh config done)
ssh site.g5k
#+END_SRC
- Access a specific site (after connection):
#+BEGIN_SRC sh
ssh site
#+END_SRC
- Copy file to site home directory :
#+BEGIN_SRC sh
scp myfile.c login@access.grid5000.fr:site/targetdirectory/mytargetfile.c
#+END_SRC
***** Resources :
- Reserve NBR hosts in interactive mode :
ASAP :
#+BEGIN_SRC sh
oarsub -l host/nodes=NBR -I
#+END_SRC
At specific time :
#+BEGIN_SRC sh
oarsub -l nodes=3 -r 'YYYY-MM-DD HH:MM:SS'
#+END_SRC
- Terminate reservation :
#+BEGIN_SRC sh
exit
#+END_SRC
- Switch hosts :
#+BEGIN_SRC sh
oarsh [name of machine I want to switch to]
#+END_SRC
- List all hosts in my reservation :
#+BEGIN_SRC sh
uniq $OAR_NODEFILE
#+END_SRC
- Delete job :
#+BEGIN_SRC sh
oardel [job id]
#+END_SRC
- Selection of resources :
#+BEGIN_SRC sh
oarsub -p "WANTED SETTINGS" -l nodes=nbr_nodes_wanted, walltime=expected_duration[HH:MM:SS]
#+END_SRC
-Extending reservation :
#+BEGIN_SRC sh
oarwalltime job_id +time_wanted
#+END_SRC
- Check detailed infos :
#+BEGIN_SRC sh
oarstat -u login -f
#+END_SRC
- TIP :
  - Avoid termination :
#+BEGIN_SRC sh
oarsub "sleep 10d"
oarsub -C ~OAR_JOB_ID~
#+END_SRC

*** 2018-05-07 Monday : Manipulations & exercices.
[[https://www.grid5000.fr/mediawiki/index.php/BigData_hands-on_tutorial][BigData hands-on tutorial]]:
#+BEGIN_SRC sh
ssh nancy.g5k
git clone https://gitlab.inria.fr/grid5000/bigdata-tutorial.git
cd bigdata-tutorial/kameleon
storage5k -a add -l chunks=3,walltime=24
pip3 install --user jupyter
echo 'export PATH=$PATH:~/.local/bin/' >> ~/.bashrc && . ~/.bashrc
jupyter notebook --ip=$(hostname -f)
#+END_SRC
G5k VPN setting : (src : Grid5000.fr)
-To start using Grid'5000 VPN, you first need to get a certificate:
Go to your account management page, select "My account" and from the "Actions" drop-down list, select "Generate VPN certificate".*
To generate a new certificate click on "Generate from Passphrase" (recommended).
If you generated your certificate and private key your self, select "Sign public key".
Your certificate will appear at the bottom of the page.
Click on "Download Files" to download an archive which includes the certificates and the configuration file needed to establish the VPN connexion.
You must extract the archive content on your workstation.
Please choose a secure place to store those files: an attacker could use them to steal your identity in Grid'5000.
From the folder :
#+BEGIN_SRC sh
sudo apt install openvpn
sudo openvpn Grid5000_VPN.ovpn
#+END_SRC
*PROBLEM* :
SOLUTION 1 -> Launching a connection via using command lines:
Changes were required to the config file :
dev tap -> dev tun
#+BEGIN_SRC sh
sudo openvpn ~Grid5000_VPN.ovpn~
#+END_SRC
Problem : The connection test fails.

SOLUTION 2 -> [[https://www.grid5000.fr/mediawiki/index.php/VPN#Launch_a_connection][Lauching a connection via the network-manager]] :
#+BEGING_SRC sh
sudo apt install network-manager-openvpn-gnome
#+END_SRC

UDP didn't work -> switching to TCP.
Both seem to be working BUT :
 ~ERR_CONNECTION_TIMED_OUT~
Connection never established!

SOLUTION 3 -> OpenSSH SOCKS proxy

Execo practical session :
Following the guide, several problems arose :
- file:Ressources/img/execo_fail.png
Solution : changing number of nodes.
-

- MPi : first manipulation of MPI on Grid5k
Without execo :
#+BEGIN_SRC sh
mpicc source.c -o app
mpirun -H graphene-23.nancy.grid5000.fr,graphene-21.nancy.grid5000.fr ./app
#+END_SRC
Output : Got 42 from rank 0
*** 2018-05-09 Wednesday
Problem solved : G5kVPN  & Jupyter work! Following same steps!
**** Pratical session part 2 MPI & Automation :
***** [[https://www.grid5000.fr/mediawiki/index.php/Run_MPI_On_Grid%275000][MPI]] :
Found a good complete tutorial on basic MPI knowledge made by Wes Kendall (src www.mpitutorial.com)
****** Running MPI -> Hello world Grid5k : See ~mpi_hello_world.c~
We reserve several nodes and get their names
#+BEGIN_SRC sh
oarsub -l nodes=2 -I
uniq $OAR_NODE_FILE
#+END_SRC
/NOTE/ : Why does it take so much time to get names?
Or with execo :
#+BEGIN_SRC python
from execo import *
from execo_g5k import *
from execo_engine import *
jobs  = oarsub([(OarSubmission("cluster=1/nodes=2", walltime=3600, job_type="allow_classic_ssh"), "lyon")])
job_id, site = jobs[0]
nodes = get_oar_job_nodes(job_id, site)
nodes
#+END_SRC

Then we need to compile and run the code
#+BEGIN_SRC sh
mpicc mpi_hello_world.c -o app
mpirun -H NAME1 NAME2 NAME3 ./app
OR
mpirun -hostfile hostfile.txt ./app
OR (if single machine)
mpirun ./app
#+END_SRC
OUTPUT : Hello world from processor graphene-66.nancy.grid5000.fr, rank 0 out of 4 processors.
****** Running MPI -> MPI locally :
#+BEGIN_SRC sh
mpicc mpi_hello_world.c -o mpi_hello_world
mpirun mpi_hello_world
OR
mpirun -hostfile hostfile mpi_hello_world
(HOSTFILE contains name of your laptop)
#+END_SRC
****** MPI [[https://www.youtube.com/watch?v=mzfVimVbguQ][Basic knowledge]] :
- MPI Communicators (1 per group of processes):
  - ~MPI_COMM_WORLD :~ All process become part of a single communicator. They can now communicate with each other
  - Rank : Each process has a unique rank (communicator). Often rank 0 is associated with master.
  - Message tag : allows differentiation between types of messages.
  - Synchronous/Asynchronous : Wait and blocking when synchronous unlike async.
  - P2P/Group communications : GC includes broadcast, reduce, scatter... functions
    - Reduce : gathering values into one of the processes by some operation.
    - Gather : gather values into a buffer in the root process.
    - Scatter : Opposite of what gather does.
    - Broadcast
****** SMPI :
- Confusing. Runs on a single node.Reduce memory footprint and simulation time.
- Testing examples from [[ https://github.com/frs69wq/simgrid.git][here]]
*PROBLEM* : can't compile :
#+BEGIN_SRC sh
smpicc -O2 -o ../bin/ep.A.1 ep.o randlc.o
/usr/bin/ld : ne peut ouvrir le fichier de sortie ../bin/ep.A.1 : Aucun fichier ou dossier de ce type
collect2: error: ld returned 1 exit status
#+END_SRC
*** 2018-05-11 Friday : SMPI
**** Since it works on Grid5k : [[https://www.grid5000.fr/mediawiki/index.php/Getting_Started#Deploying_your_nodes_to_get_root_access_and_create_your_own_experimental_environment][Short Guide]] to setting up the environnement
#+BEGIN_SRC sh
ssh site.g5k
oarsub -I -l nodes=1,walltime=3:00 -t deploy
kadeploy3 -f $OAR_NODE_FILE -e debian9-x64-base -k
ssh root@machine
sudo apt-get install simgrid git
#+END_SRC
**** First programm :
We imagine a first program with a random number of process.
The goal is to make them wait until they receive a message from process with lower rank, display their identity and send message to higher rank.
file:Ressources/img/circle_desc.png
The code can be found under ~SimGrid/examples/SMPI/Circle_discussion~
*PROBLEMS ENCOUNTERED* :
****** XML file <!DOCTYPE>:
#+BEGIN_QUOTE
Invalid XML (XML input line 2, state 2): Bad declaration <!DOCTYPE platform SYSTEM "http://simgrid.gforge.inria.fr/simgrid.dtd">
If your are using a XML v3 file (check the version attribute in <platform>), please update it with ~tools/simgrid_update_xml.pl~
#+END_QUOTE
******* Solution : Weirdest problem. What I did is that I changed version to 3, used the ~simgrid_update_xml~ and it just worked... somehow.
****** XML file power & speed :
#+BEGIN_QUOTE
Invalid XML (XML input line 6, state 41): Bad attribute `power' in `cluster' element start tag.
Invalid XML (XML input line 5, state 41): Required attribute `speed' not set for `cluster' element.
#+END_QUOTE
******* Solution : replacing power with speed
****** ~MPI_INIT~ :
#+BEGIN_QUOTE
~[root/CRITICAL] smpi_process_data() returned nullptr. You probably gave a nullptr parameter to MPI_Init. Although it's required by MPI-2, this is currently not supported by SMPI.~
#+END_QUOTE
******* Solution : Replace ~MPI_INIT(NULL, NULL)~ with ~MPI_INIT(&argc,&argv)~

Results :
- Terminal :
file:Ressources/img/circle_term.png
- Vite :
file:Ressources/img/circle_vite.png
*** 2018-05-14 Monday : Time parallel simulation
Cf cahier de notes.
Questions :
- What kind of network?
- Compare to seq?
- Can I simulate whatever I want?
=> Will have to state who starts and when.
*** 2018-05-15 Tuesday
**** Instructions A.Legrand

  https://simgrid.github.io/SMPI_CourseWare/

Sinon, dans ton journal, je vois que tu captures (en png) des sorties de
terminaux. N'hésite pas à utiliser le support org-mode pour ça, c'est
plus pratique. Tu fais "<b" puis "tab" et ça complète en un bloc shell
que tu peux exécuter avec C-c C-c. N'hésite pas à passer me voir si tu
as besoin.

Pour résumer la discussion d'hier:
- Je te disais qu'il n'y a probablement pas besoin de déployer ton image
(c'est plus lent et ça induit des erreurs potentielles). J'ai
probablement tort. Il est probable qu'au final, ça te simplifiera la vie
car l'environnement par défaut de G5K est quand même un peu minimal. Tu
pourras plus facilement installer ce dont tu as besoin. Donc continue
comme tu as fait, c'est très bien.
- [X] Installe SimGrid à partir des sources
#+BEGIN_SRC sh
apt-get install libboost-dev libboost-context-dev
apt-get install default-jdk libgcj16-dev
apt-get install liblua5.3-dev lua5.3
#+END_SRC sh

#+RESULTS:

[[https://gforge.inria.fr/frs/?group_id=12][Download]]
#+BEGIN_SRC sh
tar xf SimGrid-3.18.tar.gz
cd SimGrid-3.18
cmake -DCMAKE_INSTALL_PREFIX=/opt/simgrid .
make
make install
#+END_SRC
- [X] Simule ep avec griffon.xml. C'est un exemple sans intérêt mais trivial
à mettre en place. Normalement, maintenant, tu sais faire ça en local
sur ta machine.
#+BEGIN_SRC sh
cd examples/smpi/NAS
make ep NPROCS=1 CLASS=S
#+END_SRC
*PROBLEM* : Unable to find the executable
*SOLUTION* : Isolate needed code (Cf A.Legrand -> Minimalistic commit)
- [X] Déploie un jupyter-notebook sur une frontend de G5K et vérifie que tu
arrives bien à t'y connecter depuis ton laptop. See Monday 07/05
#+BEGIN_SRC sh
pip3 install --user jupyter
echo 'export PATH=$PATH:~/.local/bin/' >> ~/.bashrc && . ~/.bashrc
jupyter notebook --ip=$(hostname -f)
#+END_SRC
- [X] Réserve (dans le notebook) 4 ou 5 noeuds d'un cluster.
Cf Notebook : EP.
- [X] Lance smpirun ep à coup de execo.action.Remote
- [X] récupère les sorties standard. (via ~smpi_results.txt)~
- On verra ensuite ensemble comment bien sauvegarder/organiser tout ça.

***** Note meeting with teachers :
- Ccmake, cmake, make : Cf cahier de notes.
#+begin_src shell :results output :exports both
sudo apt-get install cmake-curses-gui
#+end_src

#+RESULTS:
*** 2018-05-16 Wednesday / 2018-05-17 Thursday
The class execo.action.Remote allows us to launch a command remotely on several hosts
For each connection requested a ssh proccess is launched.
**** [[https://www.grid5000.fr/mediawiki/index.php/Screen][Screen]] + cf Meeting notes
Ctrl+a d
screen -r <id>
**** How do we get terminal output without using a file? Pistes :
***** Screen :
If we have 2 machines, lets say A and B. To have a single terminal for both, we would need to do these steps :
#+begin_src shell :results output :exports both
A : screen -q
B : ssh A
B : screen -ls
A : screen -x <id>
#+end_src
***** Ttylog
*** 2018-05-18 Friday MEETING VISIO
**** Things to do :
- [X] ~Remote_smpi_env~ don't need to be root : *PROBLEM* Without root it's executed locally
- [ ] Save image after deployment and use it next times to save time : [[http://kameleon.imag.fr/grid5000_admin_tutorial.html#creating-a-grid-5000-environment][tutorial]]
To deploy image (reserve node + deploy) :
#+begin_src shell :results output :exports both
oarsub -I t deploy
kadeploy -a my_debian.yaml -f $OAR_NODEFILE
#+end_src
- [X] Clone git repository on G5k frontend
- [X] Remote Emacs (ForwardX11) or sshfs
#+begin_src shell :results output :exports both
sshfs login@nancy.g5k:/home/nezzine ./
#+end_src
*PROBLEM* doesn't answer
#+begin_src shell :results output :exports both
mount  -t ssh
#+end_src
Allow accessing remote repositories (those on G5k) locally.
- [X] Organize & save data :
  - [X] Add date to name of smpi results
  - [X] Use better structure of repositories
  - [X] Add cell with :
#+begin_src shell :results output :exports both
git add _____.txt
git commit
#+end_src
To save results !
- [X] Versionning :
  - [X] Duplicate notebook for each experiment
#+begin_src shell :results output :exports both
git copy
#+end_src
It's going to take a lot of place locally but not on gitHub!
   - [X]  Take a few cells to indicate :
     - Experiment id
     - Git version
     - Git status
     - Software version
   - To use R : [[http://simgrid.gforge.inria.fr/contrib/R_visualization.php#org667a940][here]]
*** 2018-05-22 Tuesday :
Working on things to do from previous section.
Tip :
- When changing bashrc on frontend, use
#+begin_src shell :results output :exports both
source ~/.bashrc
#+end_src
to force reload of bashrc.

- Informations :
  - I don't install SimGrid on frontend. What should I do for smpi*** version?

*** 2018-05-23 Wednesday :
- [X] shhfs :
Same command magically works.
- [X] r on jupyter :
#+begin_src shell :results output :exports both
pip install --user rpy2
#+end_src
*PROBLEM* : "Failed building wheel for rpy2"
SOLUTION : update/upgrade r => version 3.4 (Someone to lean on)
- [ ] Broadcast pgm
*** 2018-05-24 Thursday - 2018-05-28 Monday :
- [X] Broadcast pgm :
We want to create a pgm where each process gets to send a broadcast message.
To do this we're gonna use ~MPI_BROADCAST~
*** 2018-05-30 Wednesday :
Time // simulation :

We start by creating a simple program that would a=have each nodes broadcast a certain number N of messages and make some calculations. It would more or less do this :

	 for (int i = 0 ; i<N ; i++){
		~MPI_Bcast()~
		//Some calculations
	}

It’s going to take each node some time to make all the iterations and we could represent it that way :

file:Ressources/img/tab_synth1.png 

When simulating with smpi, each nodes that’s ready is going to be asked what it could do. Then equations are going to be resolved calculating the time taken. But this is not the main purpose of this project. To make a time parallel simulation we are going to fragment it.
We are going to reserve some machines and have all the nodes work on a smaller chunk. Rather than having all of them work on 0-N, machine 1 is going to have the nodes work on 0-100 and machine 2 on 100-200 etc.

To better understand what D is about, imagine what’s happening on machine 1. All the nodes have to iterate from i = 0 to i = 100. It’s going to take node 0 a certain amount of time, and node 1 a certain amount of time that could be the same or different from node 0 and same thing for all the other nodes. Basically, when a node p is finished we’re going to save the time it took in D[p]. It’s going to represents the final state of the process.
For the first process, D equals 0 for each node, because we start from 0. For machine 2 however the answer is not that obvious. In fact, until machine 1 isn’t done we have no idea what its value is. And that’s where optimism kicks in. We ‘re just going to pretend we know and give it some random value or we could start with 0. Once machine 1 is finished we will compare and redo the simulation if needed.

file:Ressources/img/tab_synth2.png

So when machine 1 is finished we retrieve S which represents the time measured. To obtain D’, which is the next D for the corrected simulation D’[p]= S[p]-min(S)

But how do we implement this?
	First, we need a program (see broadcast.c). In this program as previously said we just broadcast and make some calculations. 
	From the notebook we are going to divide the simulation between the machines. To divide the work equally between the machines we take into account the number of machines and the number of iterations N. The minimum i is going to be ((i*N)/number of (nodes)) and max ((i+1)*N)/number of (nodes))
	The inital delay D is a null vector.
	
	On the program side the macro PARSIM retrieves parameters and controls the simulation. 
Let p be a node, so when launching the program we receive these parameters :
- Min : The i where it has to starts its iterations
- Max : The i where it has to end it's iterations
- D : explained sooner
- i : current position
If the current i is smaller we're going to move it to the min and sleep D[p]. If it’s higher than max then we’re finished, we save the value in S and write it in a file.

Back to jupyter, we need to process the data. First we need to parse the file with the results which is exactly what the parser() function does. This might be improved in the future but for now a separate function makes the conversion from the tab with raw string in arrays to an array with actual data needed, ~tab_conv()~. And finally we calculate the new D’[].
What's next?
- [ ] dynamic recalculation according to D and S
- [ ] the calls to make run are sequentials
*** 2018-05-31 Thursday :
MEETING WITH FLORENCE P.

Le script + instrumentation pour découper les séquences d'itérations
sont en place.

Le parser pour récupérer les résultats aussi. Les expériences se
lancent depuis un notebook jupyter.

Il faut:
[résolution de bugs]
- [X] résoudre le bug qui donne (0,0,...0) comme résultat pour la dernière
  machine => IS IT REALLY USEFUL? I MEAN WE DON'T REALLY CARE ABOUT THE LAST MACHINE?
From Arnaud :
Ce n'est pas un "bug" à proprement dit, mais c'est surtout
qu'effectivement, on s'en fiche. Ceci dit, il serait bien de le
capturer. Je n'ai pas de bonne idée là pour bien l'écrire mais on
demandera autour de nous à l'occasion, perd pas de temps avec ça.
Rajoute juste après la boucle le test sur la valeur de l'indice
équivalent à que tu faisais habituellement.
- [X] lancer ces séquences en parallèle (en même temps)
[new features : dans l'ordre] =>  [[https://openclassrooms.com/courses/apprenez-a-programmer-en-python/la-programmation-parallele-avec-threading][parallel python]]
- [X] après récupération du vecteur de décalage D, relancer les
  expériences en utilisant ce vecteur de décalage comme état initial
  (d'abord sur 2 noeuds, puis sur un peu +)
Done, but the values are a bit stange *TODO* : Ask whether they are coherent or not!
- [X] tester si les trajectoires se rejoignent en fin de slot
-> Used an array called coherence testing if i and i+1 have the delay ! Need to work more on this.
- [ ] tester la cohérence des trajectoires (cohérente = trajectoire
  raccrochée à la trajectoire initiale) 
- [ ] critère d'arrêt
** 2018-06 June
*** 2018-06-04 Monday :
- [X] tester si les trajectoires se rejoignent en fin de slot
-> Used an array called join testing if i and i+1 have the delay !
*Wrong!* : Need to test i-1 and i. Because in the loop data regarding i+1 is not computed yet!
*Principle :* If machines[i-1] and machines[i] have Dprime and D equal we set the value of join[i-1] to True.
- [X] tester la cohérence des trajectoires (cohérente = trajectoire
  raccrochée à la trajectoire initiale) 
To check if the path to the i^th process is coherent, we need to check it the cells of the array join from 0 to i are set to True. If yes then the path is coherent!  
- [X] critère d'arrêt
We stop if join is filled with Trues or if we simulated it len(nodes) times.

*** 2018-06-05 Tuesday :
- Seems to work. Focusing on testing and organizing code.
- Very weird values for last rounds.

MEETING with Florence P. :
Les expériences avec récupération d'états et tests de cohérence sont codées.
Il reste quelques petits bugs à corriger : 
- comparaison des états D[ ] pas dans le même format => vérifier qu'à
  chaque round, 1 trajectoire supplémentaire devient cohérente;
- délai multiplié par 1000 (environ) à chaque round => bug?

Ensuite dans l'algo de simulation parallèle on peut avoir des segments
trajectoire qui se rejoignent sans être nécessairement cohérents avec la trajectoire
initiale : on les remplacera
par d'autres segments optimistes;

Affinage de l'algo de simulation :

Au round numéro k, la machine i >=k a calculé k morceaux de
trajectoires différents pour son intervalle d'itérations; il y a donc
k vecteurs D[ ] à comparer à l'état obtenu par le segment d'itérations
précédentes

Few problems : 
- [X] S values are weird toward the end :  Way too big. Problem fixed : units were wrong (ms->s)
- [X] Format of D for join
*** 2018-06-06 Wednesday :
- Goal for today :
  - [X] Fix bug :
    - [X] Time for the last rounds (Done yesterday)
    - [X] Values for Join are incorrect! Because of the format but even after!
I decided to use a matrix of array called D of size number of round max (= nodecount) * number of nodes (= nodecount).
In D[j][i] I will put the delay for machine i in round j. Therefore if I want to test if they join I need to compare D[j-1][i] the delay I computed my path with and the delay the previous machine ended with, which equals delayer(i-1,...)
  - [ ] Get last value
  - [X] Comment more and change variables names to more intuitive ones
  - [X] Save all paths computed
  - [X] Compare to al previous path computed : Piste ~test_join~ 
We retrieve all previous values computed using zip(*D)[i] Then we compare each value. we compute it for 0 and then we get the final value with "or" on following tests.
Need to find another solution
*** 2018-06-07 Thursday :
- [ ] CR à F.Boyer
Changing the way we process information is going to change a lot in our code.
Previously, each machine processed a part of the iterations, and we tried to match the initial state of i to the final state of i-1. The matching was saved in an array join.
Now we want to save all the paths computed and not only keep the correct ones.

Saving paths computed is pretty easy. We use an array called D[Round][machine] to store the D on each round for each node.

With such a tool I could have a match with path computed previously, so I need to be able to spot that matching and join has to tell me wheter there is or not a path computed previously matching my current trajectory.
Currently, we have the answer to that question using 
~test_join[i-1] = any(all(x1 == x2 for x1, x2 in zip(delay,format_cmd(tab[k]))) for k in range(j))~
So for each machine, we test the D of each round to the final state, or initial state expected.
However, even if we have a match here we don’t know which path matched ! This will be further improved very soon.
Second, there are several strategies :
- I could decide at each round if it’s not a match, the new intial state is going to be the previous machines’ final state. This strategy is very easy to put in place. At each round, only the machine who didn’t match with the initial trajectory would have to recompute, So I’d have to compute starting for the last « True » in join.
file:Ressources/img/simu_fixe.jpg
- But I could also decide to randomly chose a new initial state. If so then when in round k, the initial state won’t be chosen randomly as the true initial state is available. We’ll the test if there’s a matching and we’ll need to know which path matched and continue computation at the end of the path that matched. Path could be from different rounds.
file:Ressources/img/simu_random.jpg

- Currently I can detect if a final state matches an intial state, but I can't know which one -> Change it so I can get the index
- ith process must keep on computing as long as the trajectory to ith process is not coherent
- Forcer le départ= de l'état final pour un processus i au round i parce que c'est forcément la bonne trajectoire
*** 2018-06-08 Friday :
file:Ressources/img/path.jpg
*** 2018-06-11 Monday :
- [ ] Why do we always have to restart the next simulation using the final state of i-1 as the new initial state? Of course if we're at round n then we have to do this for machine n because it is the right path. But that's not true for all machines above n.
So we're going to use modes : Fix, Random, random inferior, random superior. The "fix" mode is what we did up until now : initial state = final state. Random is where the new initial state is generated randomly. Random inferior is where the new inital state is generated randomly with the previous machine final state as a upper limit. Random superior is the with lower limit.
- [ ] Add row to D : When I first created D I gave it a j*nodecount size. D is supposed to store all data regarding delays.
Let's take a example to show what's wrong with such a size : We'll take 2 machines so 2 rounds.
In D[0][machine-x], I'm going to store the initial D. 
In D[1][machine-x], what do I store? The final state of machine[i-1]? Or the new initial state? If we are in "FIX" mode then those final = initial so there's no problem.
However if we use another mode, those are different!
Here's the solution : We need to know D[j][i] as the final state off i-1 to compute path[] values. In other words to know if there's a coherent path. So we could set the value of D[j][i] to final of i-1 then compute path then if a path wasn't found set it to a value according to the mode.
**** MEETING :
Re-définition de la structure D mémorisant les décalages:
Notation:
les $t_i$ sont les itérations de l'application parallèle initiale
les $p_i$ les processeurs virtuels (simulés)
les $q_i$ les processeurs réels utilis;es pour la simulation parallèle
$r_k$ round numéro k
$T_{t_1,t_2}$ le temps entre le proc (virtuel) de décalage nul au temps $t_1$
et celui de décalage nul (le 1er qui a fini) au temps $t_2$. Pour
avoir le temps total d'exécution de l'application parallèle il faudra
donc faire \sum T_{t_i,t_{i+1}}$ + decalage final.

Les slots doivent faire toujours la même durée (même si elle peut
varier d'un slot à l'autre) donc la structure de donnée pour recoller
les morceaux est en 2 parties :
$l: t_1 \rightarrow t_2$ (1 seul $T_2$ possible pour un $t_1$)
$sim: t_1,D_1 \rightarrow D_2 [optionnellement: (T_{t1,t2},r,q)]$

$t_curr, D_curr$.

Ensuite il y aura plusieurs heuristiques à regarder:
- choix des tailles de slots / réaffectation des processeurs qui ont
  "fini" (trajectoire cohérente) : schéma adaptatif, catch me if you
  can... 
- choix des points de départs (0 au début, puis le D2 du slot
  précédent, mais si on fait plus de trajectoires?)
- structure ~To_explore~ {(t,D)} des fragments de trajectoire à
  calculer.
On doit quand même vérifier si ce segment doit vraiment être exploré
(peut avoir déjà été couvert par des trajectoires qui fusionnent)
*** 2018-06-12 Tuesday :
t_curr is the current position of the coherent path
D_curr is the current delay associated to that coherent path
file:Ressources/img/t_cur_D_curr.png
Therefore we initialize t_curr to 0 and D_curr to '0...0'
Once we've set the fragment size we can compute the size of the last slot.
The size of the last slot equals N(=number of iterations)% ~fragment_size~

Now that we fragmented our environnement for the simulation we need to configure our machines and to decide which paths to explore.
The paths to explore are going to be stored in a structure of type collections.dequeue. Since we're going to take tasks FIFO, that structure semmed to be the more suited.

How many machines do we need to configure?
If I have more paths to explore than machines available then we should configure all of them. Otherwise we are only going to configure enough machines to work on all the task left. Therefore we just need to take the min of these 2.

The MIN the machine has to work on is retrievable from ~To_explore~. The MAX can easily be computed adding the fragment size

After launching and ending the simulation we need to process the gathered data.
First we need to compute the new D : Dprime. This can be done using the delayer function.
Then, we need to save those paths computed. We'll use nested dictionaries for that.
sim = { t : {D : (Dprime,round,q)}
file:Ressources/img/sim.png

Once Dprime is computed we just add it to sim and for now to new paths to explore 
*** 2018-06-13 Wednesay : 
What is Tt1-t2?
We need it to know how much time the simulation took.
- [ ] Chercher outil pour dessiner graphes complexes.
file:Ressources/img/Tt1-t2.jpg
PGM : 
We'll use variable ~total_execution_time~ initialized to 0. And every time we move ~t_curr~ up we will update ~execution_time_value~.
T is stored in our dict sim as the 4th field and is compute at each round. 
We changes our delayer function so we could get more data back when calling it.
Now it returns a list of D and min(S). We could have returned S but it has not been done that way to minimize further data process and because we only need min(S).
*** 2018-06-14 Thursday :
- [X] Check if the way I compute T is correct
To compute ~total_execution_time~ we add min(S) of each coherent path and max(D) of the last paht computed
**** Configuring the simulation :
To study parallel simulation let's add a few parameters :
***** Initial state :
For all the fragments except for the first one why should it always be 0? Let's randomly select it
***** Modes :
Again, what if limiting ourselves to restarting from previous fragment final state was actually, lowering the probability of computing a correct path?
We'll create 4 modes (see description with previous structure :  2018-06-07)
- [X] FIX
- [X] RAND
- [X] RAND SUP
- [X] RAND INF
***** Options :
****** Catch me if you can.
*** 2018-06-18 Monday :
Problems solved : 
- [X] if the size of the last slot is different, the final incrementation has to take it into account!
min(~t_curr+fragment_size~,N)
- [-] check parameters are correct!
  - [X] ~fragment_size~ < N
  - [ ] Correct modes
A few problems with the new ways of creating initial states. Here is what what we are trying to do:
file:Ressources/img/loop.png
- What should the upper limit of the Random be? Twice max of Dprime
- Infinite loop : /!\ Initial state of the path linked to initial one has to be added with FIX mode
*** 2018-06-19 Tuesday :
- [X] infinite loop : At some point ~To_explore~ becomes empty. Wrong condition : if(MAX!=t_curr) should be if(t!=t_curr)
- [X] testing value that doesn't exists yet : Dprime in sim[MAX]
Solution : We will use a boolean variable to test if the final path obtained has already been computed before. To solve the problem of testing a value that doesn't exist yet we'll just use exceptions as following :
try :
   caught = Dprime in sim[MAX]
except KeyError:
   caught = False        
*** 2018-06-20 Wednesday :
After several tests it seems all the different ways of generating a Dprime work. But there's a lot of data displayed so :
To make it more readable we will start using colors, functions for repetitive lines.
printc print a string with with the wanted color and background. A wrong entry will just print it black on a white background.
~generate_Dprime~ compute the Dprime given a lower and upper border. Problem was the lower border could either be an int or a list we'd need to process.
I just separated the two cases.
*QUESTION* What's better ? Having less lines possible to make it more readable of add a few lines to save a few computations?
It's still very hard to actually know if the time parallel simulation was efficient. When fragments size is small I can just read it but when it becomes bigger it's a bit hard to know.
*** 2018-06-21 Thursday :
First we are going to lower the catch me check frequency because obviously at some point there are going to be too many paths to compare and this is going to make us lose efficiency. Check every 2^k.
I used a variable ~catch_me_ind~ so I can know when to check if something was caught indeed. And if I give the right to check if something was caught I increment the variable ~catch_me_ind~
*QUESTION* To improve the probability of computing a correct initial state why not trying to learn how much time each process approximately takes? A bit like "rock paper scissors" homework?
Solving yesterday's problem to see the time parallel simulation is efficient or not, we use to variables :
- ~merged_paths~ : At the end of a computation session how many new fragments are connected to the coherent paths? If it's more than one then time parallel simulation actually made us gain in efficiency and it is a success! here comes the second variable :
- success : Initially set to False, if at some point more than 2 fragments were connected to coherent path then it is a success and we set it to True. This variable is displayed at the end.
Now how could we improve : ~To_explore~ is for the moment a rather silly structure. We just analyze what we're told to without really thinking. But what is really worth analysing? 
Because, this way of computing can make us waste time in the worst case because it requires even more rounds than a sequential computation
file:Ressources/img/worst_case_dumb_toexplore.png
"Ideas" How we could improve it :
- Always compute the coherent path. That way in the worst case, it'd be a sequential simulation
file:Ressources/img/worst_case_prefetch.png
- Improve probability of guessing the right D :
  - Within and outside of a simulation, save and learn the Ds to make better guessing.
  - Not always starting from '0...0', I mean what's the odd of such a thing happening?
  - Adjust bounds at each round
[[http://www.lacl.fr/pekergin/checkbound/transparentspresentations/checkParallelSimul.pdf][Lecture 1]]
[[http://athene-forschung.unibw.de/doc/86163/86163.pdf][Lecture 2]]
*** 2018-06-22 Friday : 
Very weird day : Impossible to deploy env.
#+BEGIN_QUOTE 
Invalid file : Unable to get the checksum
OR
Invalid request on kadeplou.rennes.grid5000.fr:25300
#+END_QUOTE
Tried : restart Kernel, and relaunching -> Same
SOLVED : Instead of using file deploy with the name of the env. 
=> This resolve the deploy pb but can't install anything with it! Which is weird!
Last try : Restart Kernel, relaunch deploy with file -> It works. What happened? Why? ~T_T~
**** MEETING :
- MISUNDERSTANDING : (Cf 2018-06-21) When adding a task to ~To_explore~ indeed we do not append it but add it to head (append left)!
- Broadcast.c :
  - Increase precision %g to get a value different from 0
  - Sort sim
  - When testing use more NPROCS
  - Remove sleeps
  - Send more (inc buff)
*** 2018-06-25 Monday :
**** TASKS :
- [-] Appendleft When adding a task to ~To_explore~ indeed we do not append it but add it to head (append left)!
  - [X] Appendleft
  - [ ] sorted
- Broadcast.c :
  - [X] Increase precision %g to get a value different from 0
  - [ ] Sort sim
  - [X] When testing use more NPROCS
  - [X] Remove sleeps
  - [ ] Send more (inc buff)
- [X] Termcolor -> Easier!
*PROBLEM* : Deleted by mistake important packages! 
*** 2018-06-26 Tuesday :
Problem resolved :
- Using connection that doesn't require authentification.
#+BEGIN_SRC sh 
wget http://91.189.91.13/ubuntu/pool/main/r/resolvconf/resolvconf_1.69ubuntu1_all.deb
dpkg -i resolvconf_1.69ubuntu1_all.deb
reboot

dpkg-reconfigure resolvconf
reboot

sudo rm /etc/resolv.conf
sudo ln -s ../run/resolvconf/resolv.conf /etc/resolv.conf
sudo resolvconf -u
#+END_SRC

Now I need to adapt code:
- convert int to float manipulation
- Reduce precision (more readable)
*** 2018-06-27 Wednesday :
 A nouveau : Problème de résolution de noms (DNS Err name not resolved).
-> Réinstallation des packages (resolvconf/openresolv) -> Résolu!
**** Apport de modifications -> Illisible : Travail sur la précision.
Nouvelle variable precision permettant de régler la précision voulue.
Mais : à l'avenir, modifier pgm pour envoi de données plus conséquentes -> Toujours trop petit même avec une précision à 4! 
How ? 
Round()
print("%.2f"%value)
**** Résolution du bug : 
file:Ressources/img/bug_solution.png
**** Appendleft mais en décroissant!
Plusieurs solutions :
file:/Ressources/img/decreasing.png
***** revered(range()) : Méthode dite plus élégante
***** range(_,_,-1)
**** Triage
- de ~To_explore~ ne pas ajouter plusieurs pistes pour une même zone, 
- supprimer les entrées <t_curr car inutiles
*** 2018-06-28 Thursday :
-> Very few machines available (Maintenance)
How to increase D? 
